{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first version of read in file for the NMX work flow (from simulatet event data to binned event data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook takes a NEXUS file from a file_type simulation and performs the steps of binning and later one from Time of Arrival (TOA) to Time of Flight (TOF) and writes out a nexus file readable by DIALS (FormartNMX) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "first add neded libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "import tracemalloc\n",
    "    #from scippnexus import data\n",
    "#import scippnexus as snx\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "    #qimport mantid_args\n",
    "#from scippnexus import NXdetector\n",
    "from scippneutron.conversion import graph\n",
    "\n",
    "import scipp as sc\n",
    "import scippneutron as scn\n",
    "import scippnexus as snx\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#%matplotlib widget\n",
    "\n",
    "\n",
    "import plopp as pp\n",
    "# pp.patch_scipp()\n",
    "import matplotlib.pyplot as plt\n",
    "from plopp.widgets import Box\n",
    "import ipywidgets as ipw\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "    #import multiprocessing as mp\n",
    "\n",
    "import h5py\n",
    "h5py.enable_ipython_completer()\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "# starting the momory monitoring\n",
    "tracemalloc.start()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define general parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number op pixels per detecotr dimmension\n",
    "pix = 1280\n",
    "pix_step = 100\n",
    "#number of time bins \n",
    "t_step = 1\n",
    "\n",
    "#number of detectors\n",
    "n_det = 1\n",
    "# normalising highest propability to \n",
    "max_prop = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datapoints in output:\n",
    "print(\"datapoint in output:\",pix**2*3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretreatment to make it faster <br>\n",
    "h5repack -l CHUNK=1024x6 2e11.h5 2e11-rechunk.h5  <br>\n",
    "or <br>\n",
    "h5repack -l CHUNK=NONE 2e11.h5 2e11-nochunk.h5<br>\n",
    "or <br> \n",
    "h5repack -l CHUNK=1024x6 mccode.h5 mccode-nochunk.h5 \n",
    "\n",
    "not neded with new McStas versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data filename must be the exact file to use for this step of data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justinbergmann/work_flow/test_data/pulse5_z.h5\n"
     ]
    }
   ],
   "source": [
    "fname = 23\n",
    "\n",
    "if fname == 1:\n",
    "   filename = '/Users/justinbergmann/work_flow/test_data/2e11.h5'\n",
    "elif fname == 2:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/x0_3d.h5'\n",
    "elif fname == 3:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/xe_0_NMX.h5'\n",
    "elif fname == 4:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/xe_20_NMX.h5'\n",
    "elif fname == 5:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/flip_det.h5'\n",
    "elif fname == 6:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/flip_4.h5'\n",
    "elif fname == 7:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/ye_0_NMX.h5'\n",
    "elif fname == 8:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/xe1_12_NMX/xe1_12_no_chunk.h5'\n",
    "elif fname == 9:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/1d.h5'\n",
    "\n",
    "elif fname == 10:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/xflat20.h5'\n",
    "\n",
    "elif fname == 11:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/small_vis/flat.nxs'\n",
    "\n",
    "elif fname == 12:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/small_vis/shift_down.nxs'\n",
    "\n",
    "\n",
    "elif fname == 13:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/mc2_7_2_test.h5'\n",
    "\n",
    "elif fname == 14:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/mc2_new_comp.nxs'\n",
    "\n",
    "elif fname == 15:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/xflat20.h5'\n",
    "\n",
    "\n",
    "elif fname == 16:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/flat_h_r.h5'\n",
    "\n",
    "\n",
    "elif fname == 17:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/flat_v_u.h5'\n",
    "\n",
    "elif fname == 18:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/NMX_long.h5'\n",
    "\n",
    "elif fname == 19:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/pulse5.h5'\n",
    "\n",
    "\n",
    "\n",
    "elif fname == 20:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/pulse5_y.h5'\n",
    "elif fname == 21:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/wavelength.h5'\n",
    "\n",
    "elif fname == 22:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/wavelength_short1.h5'\n",
    "elif fname == 23:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/pulse5_z.h5'\n",
    "elif fname == 24:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/nexus-v3_2-with_detectors_stageZ_and_one_axis5.h5'\n",
    "\n",
    "else:\n",
    "    filename = '/Users/justinbergmann/work_flow/test_data/xflat20.h5'\n",
    "\n",
    "\n",
    "# f_vis = '/Users/justinbergmann/work_flow/test_data/ye_0_NMX_vis.nxs'\n",
    "# f_vis ='/Users/justinbergmann/work_flow/test_data/mc2_new_comp_vis.nxs' \n",
    "# f_vis ='/Users/justinbergmann/work_flow/test_data/flat_v_u_vis.nxs'\n",
    "f_vis ='/Users/justinbergmann/work_flow/test_data/pulse5_z_vis.nxs'  \n",
    "print(filename)\n",
    "\n",
    "\n",
    "#f=snx.File(filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define needed fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDist2(A,B):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two points.\n",
    "    \"\"\"\n",
    "    dist = len3dvec(twoP_to_vec(A, B))\n",
    "    return dist\n",
    "\n",
    "def len3dvec(vec):\n",
    "    \"\"\"\n",
    "    Calculate the length of a 3D vector.\n",
    "    \"\"\"\n",
    "    a = np.sqrt(vec[0]**2 + vec[1]**2 + vec[2]**2)\n",
    "    return a\n",
    "\n",
    "def twoP_to_vec(A,B):\n",
    "    \"\"\"\n",
    "    Create a vector between two points.\n",
    "    \"\"\"\n",
    "    vec = np.array([B[0]-A[0], B[1]-A[1], B[2]-A[2]])\n",
    "    return vec\n",
    "\n",
    "def rotation_matrix(axis, theta):\n",
    "    \"\"\"\n",
    "    Return the rotation matrix associated with counterclockwise rotation about\n",
    "    the given axis by theta radians.\n",
    "    \"\"\"\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis / math.sqrt(np.dot(axis, axis))\n",
    "    a = math.cos(theta / 2.0)\n",
    "    b, c, d = -axis * math.sin(theta / 2.0)\n",
    "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "    return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "                     [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "                     [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
    "\n",
    "def get_data_path(file):\n",
    "    \"\"\" try to find data in file\"\"\"\n",
    "    file_type = \"none\"\n",
    "    try:\n",
    "        a = file['entry1/data']['bank01_events_dat_list_p_x_y_n_id_t']['events'][...]\n",
    "        file_type = 'mcstas'\n",
    "    except:\n",
    "        try:\n",
    "            a = file['entry1/data']['bank01_events_dat_list_p_x_y_n_id_t_L_L']['events'][...]\n",
    "            file_type = 'mcstas_L'\n",
    "        except:\n",
    "            try:\n",
    "                a = file['/entry/instrument/detector_panel_0/event_data/']['event_time_offset'][...]\n",
    "                file_type = 'NMX_nexus'\n",
    "            except:\n",
    "                print(\"do data found in file\")\n",
    "                file_type = 'none'\n",
    "    return a, file_type\n",
    "\n",
    "def get_ids(file_type):\n",
    "    \"\"\"pixel IDs for each detector\"\"\"\n",
    "    if file_type == 'mcstas' or file_type == 'mcstas_L':\n",
    "        ids1 = sc.arange('id', 1, 1638401, unit=None)\n",
    "        ids2 = sc.arange('id', 2000001, 3638401, unit=None)\n",
    "        ids3 = sc.arange('id', 4000001, 5638401, unit=None)\n",
    "        \n",
    "    else:\n",
    "        ids1 = sc.arange('id', 1, 1638401, unit=None)\n",
    "        ids2 = sc.arange('id', 1638401, 3276802, unit=None)\n",
    "        ids3 = sc.arange('id', 3276802, 4915203, unit=None)\n",
    "\n",
    "    ids = sc.concat([ids1, ids2, ids3], 'id')\n",
    "    return ids\n",
    "\n",
    "def get_events_mcstas(a, file_type, max_prop):\n",
    "    \"\"\"get event list depending on dataformart and return dataset\"\"\"\n",
    "    if file_type == 'mcstas' or file_type == 'mcstas_L':\n",
    "        d = np.matrix.transpose(a)\n",
    "        print(\"shape of event list (p_x_y_n_id_t)\", d.shape)\n",
    "        print(\"start extracting data\")\n",
    "        t_list = sc.array(dims=['x'], unit='s', values=d[5])\n",
    "        id_list = sc.array(dims=['x'], unit=None, values=d[4], dtype='int64')\n",
    "        weights = sc.array(dims=['x'], unit='counts', values=d[0]) #change to integer for measured data\n",
    "        # Normalize weights to the maximum probability\n",
    "        weights = weights * (max_prop/weights.max()) #delete for actual data\n",
    "        # weights = sc.ones_like(x_list)\n",
    "        # weights.unit = 'counts'\n",
    "        print(\"tlist range\",t_list.min(), t_list.max())\n",
    "        da = sc.DataArray(data=weights, coords={ 't': t_list, 'id': id_list})\n",
    "        # Ensure all IDs are recognized\n",
    "        print(\"id min\",id_list.values.min())\n",
    "        print(\"id max\",id_list.values.max())\n",
    "    \n",
    "    return da, weights\n",
    "\n",
    "\n",
    "\n",
    "def read_in_data(filename):\n",
    "    \"\"\"\n",
    "    Read data from an HDF5 file and prepare it for processing.\n",
    "    \"\"\"\n",
    "    f = h5py.File(filename)\n",
    "    a, file_type = get_data_path(file=f)\n",
    "    if file_type == 'mcstas' or file_type == 'mcstas_L':\n",
    "        # Allocate units to events and create separate lists for each parameter\n",
    "        da, weights = get_events_mcstas(a, file_type, max_prop)\n",
    "    elif file_type == 'NMX_nexus':\n",
    "        da, weights = get_events_NMX_nexus(file=f)\n",
    "\n",
    "    #print(x_list.shape, y_list.shape, t_list.shape,id_list.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    ids = get_ids(file_type)\n",
    "    \n",
    "    return da, ids, file_type, weights\n",
    "\n",
    "def get_instrument_geometry(filename,file_type):\n",
    "    \"\"\" gether geometry data about the instrument\"\"\"\n",
    "    if file_type== 'mcstas' or file_type=='mcstas_L':\n",
    "        ds_l,fast_l,slow_l, sample_pos, source_pos = get_mcstas_geo(filename)\n",
    "\n",
    "    else:\n",
    "        ds_l,fast_l,slow_l, sample_pos, source_pos = get_default_parameters()\n",
    "\n",
    "    print(\"sample position\",sample_pos)\n",
    "    print(\"source position\",source_pos)\n",
    "    print(\"relative to sample position\",ds_l)\n",
    "    print(\"fast axis\",fast_l)\n",
    "    print(\"slow axis\", slow_l)\n",
    "    return ds_l,fast_l,slow_l, sample_pos, source_pos\n",
    "\n",
    "\n",
    "def get_events_NMX_nexus(file):\n",
    "    \"\"\"gether data for all detector panels \"\"\"\n",
    "    da_l =[]\n",
    "    w_l = []\n",
    "    for i in range(n_det):\n",
    "        da , weights = get_events_NMX_nexus_panel(file,i)\n",
    "        da_l .append(da)\n",
    "        w_l.append(weights)\n",
    "    data_l = sc.concat(da_l, 'x')\n",
    "    waight_l = sc.concat(w_l, 'x')\n",
    "\n",
    "    return data_l, waight_l\n",
    "    \n",
    "def get_events_NMX_nexus_panel(file, det_number):\n",
    "    \n",
    "    \"\"\"get event list depending for NMX nexus formart\"\"\"\n",
    "    d_path = '/entry/instrument/detector_panel_'+str(det_number)+'/event_data'\n",
    "    print(\"data path\",file[d_path]['event_id'])\n",
    "    id_read = file[d_path]['event_id'][...]\n",
    "    t_read = file[d_path]['event_time_offset'][...]\n",
    "    print(\"event time offset\",t_read)\n",
    "    t0_index_read = file[d_path]['event_index'][...] \n",
    "    t0_read = file[d_path]['event_time_zero'][...]\n",
    "    print(\"event_time_zero\",t0_read)\n",
    "    print(\"t\",t_read.shape)\n",
    "    print(\"t0\",t0_read.shape)\n",
    "    print(\"id\",id_read.shape)\n",
    "    t_d = t_read.astype(float)*10**9\n",
    "    t0_d = t0_read.astype(float)\n",
    "    id_d = id_read.astype(int)\n",
    "    t0_index_d = t0_index_read.astype(int)\n",
    "    print(\"t_d min max\", min(t_d), max(t_d))\n",
    "    print(\"t0_d min max\", min(t0_d), max(t0_d))\n",
    "    # t_d = np.matrix.transpose(t_read)\n",
    "    #print(\"shape of event list (p_x_y_n_id_t)\", d.shape)\n",
    "    #print(\"start extracting data\")\n",
    "    # print(\"t_d\",t_d.shape)\n",
    "\n",
    "    ## transfere from event_time_offset, event_time_zero, event_index  to TOF\n",
    "\n",
    "    t_list = sc.array(dims=['x'], unit='ns', values=t_d, dtype='int32')\n",
    "    # t0_list = sc.array(dims=['x'], unit='ns', values=t0_d, dtype='int64')\n",
    "    id_list = sc.array(dims=['x'], unit=None, values=id_d, dtype='int64')\n",
    "\n",
    "    weights = sc.ones_like(t_list)\n",
    "    weights.unit = 'counts'\n",
    "    da = sc.DataArray(data=weights, coords={ 't': t_list, 'id': id_list})\n",
    "    # Ensure all IDs are recognized\n",
    "    #print(\"id min\",id_list.values.min())\n",
    "    #print(\"id max\",id_list.values.max())\n",
    "    \n",
    "\n",
    "    return da, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_number = 1\n",
    "d_path = '/entry/instrument/detector_panel_'+str(det_number)+'/event_data'\n",
    "print(d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_results(filename, ds_l,fast_l,slow_l, sample_pos, source_pos,group_t, t_step, weights ):\n",
    "\n",
    "    #create name of outputfile\n",
    "    no = filename.split('/')\n",
    "    # print(no)\n",
    "    name_out= no[-1].split('.')[0]\n",
    "    print(name_out)\n",
    "    file_out = '/Users/justinbergmann/work_flow/test_out/'+name_out+'_'+str(t_step)+'_out.h5'\n",
    "    print(file_out)\n",
    "    with h5py.File(file_out, 'w') as fo:\n",
    "## create output nexus file\n",
    "      fo.attrs['default'] = 'NMX_data'\n",
    "      nxentry = fo.create_group('NMX_data')\n",
    "      nxentry.attrs[\"NX_class\"] = 'NXentry'\n",
    "      nxentry.attrs['default'] = 'data'\n",
    "      nxentry.attrs['name'] = 'NMX1'\n",
    "      #nxentry.__setitem__('beamline','NMX')\n",
    "      nxentry.__setitem__('name','NMX')\n",
    "      nxentry.__setitem__('definition','TOFRAW')\n",
    "      nxentry.attrs['name'] = \"NMX\"\n",
    "      #nxentry.attrs['name'].__setattr__('name','NMX') \n",
    "\n",
    "#SAMPLE\n",
    "      nx_sample = nxentry.create_group('NXsample')\n",
    "      nx_sample.__setitem__('name','Single_crystal')\n",
    "\n",
    "\n",
    "#Instrument\n",
    "      nx_instrument = nxentry.create_group('NXinstrument')\n",
    " \n",
    "      nx_detector = nxentry.create_group('NXdetector')\n",
    "      det_origen = nx_detector.create_dataset('origen', data=ds_l) \n",
    "      det_origen.attrs['units'] = 'm'\n",
    " \n",
    "      fast_axis = nx_detector.create_dataset('fast_axis', data=fast_l) \n",
    "      slow_axis = nx_detector.create_dataset('slow_axis', data=slow_l) \n",
    " \n",
    "      nx_beam = nxentry.create_group('NXbeam')\n",
    " \n",
    " \n",
    "      \n",
    "      proton = nxentry.create_dataset('proton_charge', data=weights.shape[0]/10000)    \n",
    "      \n",
    "      \n",
    "      nx_det1 = nxentry.create_group('detector_1')   \n",
    "      counts = nx_det1.create_dataset('counts', data=[group_t.values], compression=\"gzip\", compression_opts=4)\n",
    " \n",
    "      t_spectrum = nx_det1.create_dataset('t_bin', data=group_t.coords['t'].values, compression=\"gzip\", compression_opts=4)\n",
    "      t_spectrum.attrs['units'] = 'ms'\n",
    "      t_spectrum.attrs['long_name'] = 't_bin TOF (ms)'\n",
    " \n",
    "      pixel_id = nx_det1.create_dataset('pix_id', data=group_t.coords['id'].values, compression=\"gzip\", compression_opts=4)\n",
    "      pixel_id.attrs['units'] = ''\n",
    "      pixel_id.attrs['long_name'] = 'pixel ID'\n",
    "\n",
    "\n",
    "#SOURCE\n",
    "      nx_inst = nxentry.create_group('instrument')\n",
    "      nx_inst.attrs['nr_detector'] = len(fast_l)\n",
    "      nx_source = nxentry.create_group('NXsource')\n",
    "      nx_source.__setitem__('name','European Spallation Source')\n",
    "      nx_source.__setitem__('short_name','ESS')\n",
    "      nx_source.__setitem__('type','Spallation Neutron Source')\n",
    "      nx_source.__setitem__('distance',-(CDist2(source_pos, sample_pos)))\n",
    "      nx_source.__setitem__('probe','neutron')\n",
    "      nx_source.__setitem__('target_material','W')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    c_or = nxinst.create_dataset('crystal_orientation',data=cryst_or)\n",
    "#    c_or.attrs['unit'] = 'degrees'\n",
    "#    c_or.attrs['long_name'] = 'crystal orientation in Phi (degrees)'\n",
    "    \n",
    "\n",
    "    fo.close()\n",
    "    del fo\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_parameters():\n",
    "    \"\"\" define standard parameter if no other parameters are specified\"\"\"\n",
    "\n",
    "    source_pos = [0,0,0]\n",
    "    sample_pos = [ -0.53123,   -0.,      -157.405  ]\n",
    "    ds_l = [[ 0.25,  0.25, -0.29], [ 0.29,  0.25, -0.25], [-0.29,  0.25,  0.25]]\n",
    "    fast_l = [[-1.  ,  0.  , -0.01], [-0.01,  0.  ,  1.  ], [ 0.01,  0.  , -1.  ]]\n",
    "    slow_l = [[ 0., -1.,  0.], [ 0., -1.,  0.],[ 0., -1.,  0.]]\n",
    "    return ds_l,fast_l,slow_l, sample_pos, source_pos\n",
    "\n",
    "\n",
    "def get_mcstas_geo(filename):\n",
    "    \"\"\" gets all gemoetry parameter from mcstas file\"\"\"\n",
    "\n",
    "    det = False\n",
    "    source = False\n",
    "    sample = False\n",
    "    sample_pos = [0,0,0]\n",
    "    source_pos = [0,0,0]\n",
    "    d_list = []\n",
    "    rot_l1 = []\n",
    "    fast_l = []\n",
    "    slow_l = []\n",
    "\n",
    "    f = h5py.File(filename)\n",
    "    xml  = str(f['entry1/instrument/instrument_xml/data'][...][0]).split('\\\\n')\n",
    "    comp = False\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(xml)):\n",
    "        ls = xml[i].replace('<t',' ').replace('>',' ').replace('\"',' ').replace('<',' ').replace('\\\\t',' ').split()\n",
    "        # print(xml[i])\n",
    "        \n",
    "        if len(ls) >= 1:\n",
    "            if ls[0] == 'component':\n",
    "                det = False\n",
    "                source = False\n",
    "                sample = False\n",
    "                comp = True\n",
    "                if ls[2].split('-')[0] == 'MonNDtype':\n",
    " \n",
    "                    det = True\n",
    "                    d_list.append([int(ls[2].split('-')[1])])\n",
    "                elif ls[2] == 'sourceMantid-type':\n",
    "                    source = True\n",
    "                elif ls[2] == 'sampleMantid-type':\n",
    "                    sample = True\n",
    "                comp = True\n",
    "         #   if ls[1].split('-')[0] == 'type=\"MonNDtype':\n",
    "                # print(\"1\",ls)\n",
    "        if len(ls) >= 1:\n",
    "            if ls[0] == 'type':\n",
    "                comp = False\n",
    "        if len(ls) >= 1:\n",
    "            if ls[0] == 'location' or ls[0] == 'location':\n",
    "                # print(\"3\",ls)\n",
    "                if comp == True and det == True:\n",
    "                    # print(\"2\",ls)\n",
    "                    xyz = [float(ls[2]),float(ls[4]),float(ls[6])]\n",
    "                    # print(\"xyz\", xyz)\n",
    "                    rot =float(ls[8]) \n",
    "                    rot_xyz =[ float(ls[8]), float(ls[10]),float(ls[12]),float(ls[14])] \n",
    "                    # print(\"rotation of detector\",rot, rot_xyz) \n",
    "                    d_list[len(d_list)-1].append(xyz)\n",
    "                    rot_l1.append(rot_xyz)\n",
    "                elif comp == True and source == True: \n",
    "                  source_pos = [float(ls[2]),float(ls[4]),float(ls[6])]\n",
    "                elif comp == True and sample == True: \n",
    "                   sample_pos = np.array([float(ls[2]),float(ls[4]),float(ls[6])])\n",
    "# print(len(d_list))\n",
    "# print(\"sample and source position\",sample_pos,source_pos)\n",
    "# print(\"distance between sample and source\",CDist2(source_pos, sample_pos))\n",
    "# print(\"detector positions, relative to source at 0,0,0:\",d_list)\n",
    "# print(\"rotation list\",rot_l)\n",
    "#shift from rleative position to source to relative to sample\n",
    "    ds_l = []\n",
    "    sample_pos = sample_pos * [-1,-1,-1]\n",
    "    # print(\"sample_pos\",sample_pos)\n",
    "    for i in range(len(d_list)):\n",
    "        det_pos = np.array([d_list[i][1][0],d_list[i][1][1],d_list[i][1][2]]) * [-1.0, -1.0,-1.0]\n",
    "        \n",
    " \n",
    "        rel_xyz= np.round(twoP_to_vec(sample_pos,det_pos),2)\n",
    "        # print(\"rel_xyz\",rel_xyz)\n",
    "        rel_xyz = rel_xyz # * [-1,-1,-1]\n",
    "        # print(\"sample to detector dist\",len3dvec(rel_xyz))\n",
    "        # rel_pos= twoP_to_vec(det_pos, sample_pos)\n",
    "        # print(\"detector position\",sample_pos,det_pos,rel_pos)\n",
    "        \n",
    "        # rel_xyz = [ d_list[i][1][0]-sample_pos[0], d_list[i][1][1]-sample_pos[1], (d_list[i][1][2]-sample_pos[2])]\n",
    "        \n",
    "        # print(\"relative position\",rel_xyz)\n",
    "        # rel_xyz = [ sample_pos[0]-d_list[i][1][0],sample_pos[1]-d_list[i][1][1], sample_pos[2]-d_list[i][1][2]]\n",
    "        # print(\"position\",rel_xyz,rel_pos)\n",
    "        # print(\"relative position\",rel_xyz)\n",
    "        ds_l.append(rel_xyz)\n",
    "    vec_f = [-1,0,-0]\n",
    "    vec_s = [0,-1,0]\n",
    "    rot_l = [rot_l1[0],rot_l1[2],rot_l1[1]] #not corect but a temporary work around\n",
    "    for i in range(len(rot_l)):\n",
    "        # if rot_l[i][2] < 0:\n",
    "            # theta = np.radians(rot_l[i][0]) \n",
    "        # else:\n",
    "        theta = np.radians(-rot_l[i][0])\n",
    "        v = [rot_l[i][1],rot_l[i][2],rot_l[i][3]]\n",
    "        v1 = [0,0,1]\n",
    "        # print(\"v\",i,v,\"theta\", theta,rot_l[i][0])\n",
    "        fast_vec = np.round(np.dot(rotation_matrix(v,  theta), vec_f),2)\n",
    "        fast_v_round = np.array([np.round(fast_vec[0],1), np.round(fast_vec[1],1), np.round(fast_vec[2],1)])\n",
    " \n",
    "        fast_l.append(fast_vec)\n",
    "        slow_l.append(np.dot(rotation_matrix(v, theta), vec_s))\n",
    "        # slow_l.append([0,1,0])\n",
    "\n",
    "    # print(\"relative to sample position\",ds_l)\n",
    "    # print(\"fast axis\",fast_l)\n",
    "    # print(\"slow axis\", slow_l)\n",
    "    # print(rot_l)\n",
    "    return ds_l,fast_l,slow_l, sample_pos, source_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, ids, file_type, weights  = read_in_data(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D visualisation with file converted by mantid (must be done before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = da.group(ids)\n",
    "del da , ids\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_t = grouped.hist(t=t_step)\n",
    "del grouped\n",
    "group_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of event list (p_x_y_n_id_t) (6, 148001741)\n",
      "start extracting data\n",
      "tlist range <scipp.Variable> ()    float64              [s]  [0.0926507] <scipp.Variable> ()    float64              [s]  [0.1519]\n",
      "id min 1\n",
      "id max 5638399\n",
      "sample position [  -0.53123   -0.      -157.405  ]\n",
      "source position [0.0, 0.0, 0.0]\n",
      "relative to sample position [array([ 0.25,  0.25, -0.29]), array([ 0.29,  0.25, -0.25]), array([-0.29,  0.25,  0.25])]\n",
      "fast axis [array([-1.  ,  0.  , -0.01]), array([-0.01,  0.  ,  1.  ]), array([ 0.01,  0.  , -1.  ])]\n",
      "slow axis [array([ 0., -1.,  0.]), array([ 0., -1.,  0.]), array([ 0., -1.,  0.])]\n",
      "pulse5_z\n",
      "/Users/justinbergmann/work_flow/test_out/pulse5_z_1_out.h5\n",
      "CPU times: user 15.3 s, sys: 5.63 s, total: 21 s\n",
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#data reduction from event data to histogrammed data\n",
    "#read in data from *.h5 file\n",
    "da, ids, file_type, weights  = read_in_data(filename)\n",
    "#grouping into pixel\n",
    "grouped = da.group(ids)\n",
    "del da, ids\n",
    "#histogram in time per pixel\n",
    "group_t = grouped.hist(t=t_step)\n",
    "del grouped\n",
    "# get instument geometry \n",
    "ds_l,fast_l,slow_l, sample_pos, source_pos = get_instrument_geometry(filename,file_type)\n",
    "#write out to *h5 file wich is readable by DIALS with NMX class \n",
    "write_out_results(filename, ds_l,fast_l,slow_l, sample_pos, source_pos,group_t, t_step, weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualsisation after treatment with mantid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo for STAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of event list (p_x_y_n_id_t) (6, 148001741)\n",
      "id min 1\n",
      "id max 5638399\n",
      "CPU times: user 17 s, sys: 8.99 s, total: 26 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = h5py.File(filename)\n",
    "a = f['entry1/data']['bank01_events_dat_list_p_x_y_n_id_t']['events'][...]\n",
    "#a[0]\n",
    "d = np.matrix.transpose(a)\n",
    "print(\"shape of event list (p_x_y_n_id_t)\", d.shape)\n",
    "#alocate units to events and create seperate list for each parameter\n",
    "x_list = sc.array(dims=['x'], unit='m', values=d[1])\n",
    "y_list = sc.array(dims=['x'], unit='m', values=d[2])\n",
    "t_list = sc.array(dims=['x'], unit='s', values=d[5])\n",
    "id_list = sc.array(dims=['x'], unit=None, values=d[4], dtype='int64')\n",
    "#print(x_list.shape, y_list.shape, t_list.shape,id_list.shape)\n",
    "\n",
    "\n",
    "weights = sc.array(dims=['x'], unit='counts', values=d[0]) #change to integer for measured data\n",
    "# normalise neutron with the highest probability to set value\n",
    "weights = weights * (max_prop/weights.max()) #delete for actual data\n",
    "# weights = sc.ones_like(x_list)\n",
    "# weights.unit = 'counts'\n",
    "da = sc.DataArray(data=weights, coords={'x': x_list, 'y': y_list, 't': t_list, 'id': id_list})\n",
    "\n",
    "#make sure alll IDs are reconised:\n",
    "print(\"id min\",id_list.values.min())\n",
    "print(\"id max\",id_list.values.max())\n",
    "\n",
    "ids1 = sc.arange('id', 1, 1638401, unit=None)\n",
    "ids2 = sc.arange('id', 2000001, 3638401, unit=None)\n",
    "ids3 = sc.arange('id', 4000001, 5638401, unit=None)\n",
    "ids = sc.concat([ids1, ids2, ids3], 'id')\n",
    "#grouping by IDs\n",
    "da3 = da.group(ids).fold('id', sizes={'panel':3, 'id':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af76438e99a4266b47d0b8184787cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HBar(children=(Box(children=(InteractiveFig(children=(HBar(), HBox(children=(VBar(children=(Tool…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#binning of each detector panel in t_step bins\n",
    "panel_0 = da3['panel',0].hist(t=t_step)\n",
    "panel_1 = da3['panel',1].hist(t=t_step)\n",
    "panel_2 = da3['panel',2].hist(t=t_step)\n",
    "\n",
    "p0 = pp.slicer(da3.fold('id', sizes={'ypix':pix, 'xpix':-1})['panel', 0].hist(t=t_step).transpose(['t','ypix','xpix']),vmax=panel_0.max().value, aspect='equal',title='panel 0')\n",
    "p1 = pp.slicer(da3.fold('id', sizes={'ypix':pix, 'xpix':-1})['panel', 1].hist(t=t_step).transpose(['t','ypix','xpix']),vmax=panel_1.max().value, aspect='equal',title='panel 1')\n",
    "p2 = pp.slicer(da3.fold('id', sizes={'ypix':pix, 'xpix':-1})['panel', 2].hist(t=t_step).transpose(['t','ypix','xpix']),vmax=panel_2.max().value, aspect='equal',title='panel 2')\n",
    "\n",
    "#p0.children[0].ax.set_ylim(1280, 0)\n",
    "#p1.children[0].ax.set_ylim(1280, 0)\n",
    "#p2.children[0].ax.set_ylim(1280, 0)\n",
    "\n",
    "p0.children[0].ax.set_xlim(1280, 0)\n",
    "p1.children[0].ax.set_xlim(1280, 0)\n",
    "\n",
    "p2.children[0].ax.set_xlim(1280, 0)\n",
    "\n",
    "l1 = ipw.jslink((p0.children[1].children[0].children[1], 'value'),\n",
    "               (p1.children[1].children[0].children[1], 'value'))\n",
    "l2 = ipw.jslink((p0.children[1].children[0].children[1], 'value'),\n",
    "               (p2.children[1].children[0].children[1], 'value'))\n",
    "Box([[p2, p0, p1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14506079291a4f75be0ad406e96e1c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Figure(children=(HBar(children=(HTML(value=''),)), HBox(children=(VBar(children=(Toolbar(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "panel0_events = da3['panel', 0].bins.concat().values\n",
    "nbin= pix\n",
    "t_step = 250\n",
    "# nbin = 250\n",
    "binned = panel0_events.bin(y=nbin, x=nbin)\n",
    "#binned = da.bin(y=50, x=50)\n",
    "#binned = da.bin(y=225, x=225)\n",
    "#sc.transpose(binned)\n",
    "\n",
    "binned\n",
    "\n",
    "hist3=binned.hist(t=t_step)\n",
    "\n",
    "hist4 = hist3.copy()\n",
    "for name in list(hist4.coords.keys()):\n",
    "    hist4.coords[f'{name}c'] = sc.midpoints(hist4.coords[name])\n",
    "hist4\n",
    "\n",
    "hist5 = sc.flatten(hist4, to='row')\n",
    "ava = np.average(hist5.values) #avarage of values to use as cut off creteria\n",
    "sig = np.std(hist5.values)\n",
    "cut = ava + sig*15\n",
    "#print(\"background substraction\",cut, ava, sig)\n",
    "select = hist5.data > sc.scalar(cut, unit='dimensionless') # cut of background\n",
    "filtered = hist5[select]\n",
    "filtered.coords['tc'] *= 10\n",
    "filtered\n",
    "\n",
    "pp.scatter3d(filtered, x='xc', y='yc', z='tc', pixel_size=0.001,cmap='Greys_r',opacity=0.5)#,figsize=(550, 550))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b300efb97f54ebf89adee721e7615bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Figure(children=(HBar(children=(HTML(value=''),)), HBox(children=(VBar(children=(Toolbar(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select ranges\n",
    "da = hist5\n",
    "da.coords['tc'] *= 10\n",
    "# cut0 = ava + sig*20\n",
    "cut1 = ava + sig*40\n",
    "cut2 = ava + sig*15\n",
    "a = da[da.data > sc.scalar(cut1)]\n",
    "b = da[(da.data > sc.scalar(cut2)) & (da.data < sc.scalar(cut1))]\n",
    "# print(a,b)\n",
    "\n",
    "# Display both on the same scatter plot\n",
    "p = pp.scatter3d({'a': a, 'b': b},  x='xc', y='yc', z='tc', norm='log', pixel_size=0.001)\n",
    "\n",
    "# Extract the point clouds from the final plot and set a lower opacity on the second point cloud\n",
    "clouds = list(p[0].artists.values())\n",
    "clouds[1].opacity = 0.05\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = sc.io.open_hdf5(filename = f_vis)\n",
    "scn.instrument_view(vis.sum('tof'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in event data and shape the event list to give the number of events and creates the dataset for further modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f = h5py.File(filename)\n",
    "a = f['entry1/data']['bank01_events_dat_list_p_x_y_n_id_t']['events'][...]\n",
    "#a[0]\n",
    "d = np.matrix.transpose(a)\n",
    "print(\"shape of event list (p_x_y_n_id_t)\", d.shape)\n",
    "#alocate units to events and create seperate list for each parameter\n",
    "x_list = sc.array(dims=['x'], unit='m', values=d[1])\n",
    "y_list = sc.array(dims=['x'], unit='m', values=d[2])\n",
    "t_list = sc.array(dims=['x'], unit='s', values=d[5])\n",
    "id_list = sc.array(dims=['x'], unit=None, values=d[4], dtype='int64')\n",
    "#print(x_list.shape, y_list.shape, t_list.shape,id_list.shape)\n",
    "\n",
    "\n",
    "weights = sc.array(dims=['x'], unit='counts', values=d[0]) #change to integer for measured data\n",
    "# normalise neutron with the highest probability to set value\n",
    "weights = weights * (max_prop/weights.max()) #delete for actual data\n",
    "# weights = sc.ones_like(x_list)\n",
    "# weights.unit = 'counts'\n",
    "da = sc.DataArray(data=weights, coords={'x': x_list, 'y': y_list, 't': t_list, 'id': id_list})\n",
    "\n",
    "#make sure alll IDs are reconised:\n",
    "print(\"id min\",id_list.values.min())\n",
    "print(\"id max\",id_list.values.max())\n",
    "\n",
    "ids1 = sc.arange('id', 1, 1638401, unit=None)\n",
    "ids2 = sc.arange('id', 2000001, 3638401, unit=None)\n",
    "ids3 = sc.arange('id', 4000001, 5638401, unit=None)\n",
    "ids = sc.concat([ids1, ids2, ids3], 'id')\n",
    "#grouping by IDs\n",
    "da3 = da.group(ids).fold('id', sizes={'panel':3, 'id':-1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does the same as above but with a reading file that contains also wavelength record at the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f = h5py.File(filename)\n",
    "a = f['entry1/data']['bank01_events_dat_list_p_x_y_n_id_t_L_L']['events'][...]\n",
    "#a[0]\n",
    "d = np.matrix.transpose(a)\n",
    "print(\"shape of event list (p_x_y_n_id_t)\", d.shape)\n",
    "#alocate units to events and create seperate list for each parameter\n",
    "x_list = sc.array(dims=['x'], unit='m', values=d[1])\n",
    "y_list = sc.array(dims=['x'], unit='m', values=d[2])\n",
    "t_list = sc.array(dims=['x'], unit='s', values=d[5])\n",
    "id_list = sc.array(dims=['x'], unit=None, values=d[4], dtype='int64')\n",
    "lam_list=sc.array(dims=['x'], unit='A', values=d[6])\n",
    "#print(x_list.shape, y_list.shape, t_list.shape,id_list.shape)\n",
    "\n",
    "#get evetns to seperated lists (x,y,t,id)\n",
    "#x_list.unit = 'm'\n",
    "#y_list.unit = 'm'\n",
    "#t_list.unit = 'ms'\n",
    "weights = sc.array(dims=['x'], unit='counts', values=d[0])\n",
    "weights = weights * (100000/weights.max())\n",
    "# weights = sc.ones_like(x_list)\n",
    "# weights.unit = 'counts'\n",
    "da = sc.DataArray(data=weights, coords={'x': x_list, 'y': y_list, 't': t_list, 'id': id_list, 'labda': lam_list})\n",
    "\n",
    "#make sure alll IDs are reconised:\n",
    "print(\"id min\",id_list.values.min())\n",
    "print(\"id max\",id_list.values.max())\n",
    "\n",
    "ids1 = sc.arange('id', 1, 1638401, unit=None)\n",
    "ids2 = sc.arange('id', 2000001, 3638401, unit=None)\n",
    "ids3 = sc.arange('id', 4000001, 5638401, unit=None)\n",
    "ids = sc.concat([ids1, ids2, ids3], 'id')\n",
    "#grouping by IDs\n",
    "da3 = da.group(ids).fold('id', sizes={'panel':3, 'id':-1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min and max time in the read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"t min and t max\",t_list.min(), t_list.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lambda min and labda max in A\",lam_list.min(), lam_list.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the three detectors with changeable TOF component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning of each detector panel in t_step bins\n",
    "panel_0 = da3['panel',0].hist(t=t_step)\n",
    "panel_1 = da3['panel',1].hist(t=t_step)\n",
    "panel_2 = da3['panel',2].hist(t=t_step)\n",
    "\n",
    "p0 = pp.slicer(da3.fold('id', sizes={'ypix':pix, 'xpix':-1})['panel', 0].hist(t=t_step).transpose(['t','ypix','xpix']),vmax=panel_0.max().value, aspect='equal',title='panel 0')\n",
    "p1 = pp.slicer(da3.fold('id', sizes={'ypix':pix, 'xpix':-1})['panel', 1].hist(t=t_step).transpose(['t','ypix','xpix']),vmax=panel_1.max().value, aspect='equal',title='panel 1')\n",
    "p2 = pp.slicer(da3.fold('id', sizes={'ypix':pix, 'xpix':-1})['panel', 2].hist(t=t_step).transpose(['t','ypix','xpix']),vmax=panel_2.max().value, aspect='equal',title='panel 2')\n",
    "\n",
    "#p0.children[0].ax.set_ylim(1280, 0)\n",
    "#p1.children[0].ax.set_ylim(1280, 0)\n",
    "#p2.children[0].ax.set_ylim(1280, 0)\n",
    "\n",
    "p0.children[0].ax.set_xlim(1280, 0)\n",
    "p1.children[0].ax.set_xlim(1280, 0)\n",
    "\n",
    "p2.children[0].ax.set_xlim(1280, 0)\n",
    "\n",
    "l1 = ipw.jslink((p0.children[1].children[0].children[1], 'value'),\n",
    "               (p1.children[1].children[0].children[1], 'value'))\n",
    "l2 = ipw.jslink((p0.children[1].children[0].children[1], 'value'),\n",
    "               (p2.children[1].children[0].children[1], 'value'))\n",
    "Box([[p2, p0, p1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grouping data of each pixel to prepare for time binning (these are the data that get used to write out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = da.group(ids)\n",
    "del da , ids\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "binning data in a predefined number of time binns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_t = grouped.bins.constituents['data'].hist(t=t_step)\n",
    "del grouped\n",
    "group_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_t = grouped.hist(t=t_step)\n",
    "group_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed functions to define instrument geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_edges = sc.linspace('t', t_list.min().value, t_list.max().value, t_step+1, unit=t_list.unit)\n",
    "pix_min = ids.min().value\n",
    "pix_max = ids.max().value\n",
    "print(\"pix_min, pix_max\",pix_min, pix_max)\n",
    "# pix_bin = grouped[0:100]\n",
    "# pix_bin.hist(t=t_edges)\n",
    "# pix_grop = grouped[0:1]\n",
    "# pix_hist = pix_grop.hist(t=t_edges)\n",
    "step_data = pix_hist\n",
    "pix_per_step=int(pix_max/pix_step) \n",
    "for i in range(pix_step-1):\n",
    "    # print(i,pix_per_step*i+1,pix_per_step*(i+1))\n",
    "    pix_grop = grouped[pix_per_step*i+1:pix_per_step*(i+1)]\n",
    "    pix_hist = pix_grop.hist(t=t_edges)\n",
    "    idstep_datas = sc.concat([step_data, pix_hist], 'id')\n",
    "    # step_data = sc.merge(step_data, pix_hist)\n",
    "\n",
    "# step_data\n",
    "# pix_hist\n",
    "# pix_grop\n",
    "sc.show(pix_grop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in geometry informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(filename)\n",
    "xml  = str(f['entry1/instrument/instrument_xml/data'][...][0]).split('\\\\n')\n",
    "xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gets sample, source and detector positions. <br>\n",
    "gets fast and slow axis of detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = False\n",
    "det = False\n",
    "source = False\n",
    "sample = False\n",
    "sample_pos = [0,0,0]\n",
    "source_pos = [0,0,0]\n",
    "d_list = []\n",
    "rot_l1 = []\n",
    "fast_l = []\n",
    "slow_l = []\n",
    "\n",
    "\n",
    "for i in range(len(xml)):\n",
    "    ls = xml[i].replace('<t',' ').replace('>',' ').replace('\"',' ').replace('<',' ').replace('\\\\t',' ').split()\n",
    "    # print(xml[i])\n",
    "    \n",
    "    if len(ls) >= 1:\n",
    "        if ls[0] == 'component':\n",
    "            det = False\n",
    "            source = False\n",
    "            sample = False\n",
    "            comp = True\n",
    "            if ls[2].split('-')[0] == 'MonNDtype':\n",
    "\n",
    "                det = True\n",
    "                d_list.append([int(ls[2].split('-')[1])])\n",
    "            elif ls[2] == 'sourceMantid-type':\n",
    "                source = True\n",
    "            elif ls[2] == 'sampleMantid-type':\n",
    "                sample = True\n",
    "            comp = True\n",
    "     #   if ls[1].split('-')[0] == 'type=\"MonNDtype':\n",
    "            # print(\"1\",ls)\n",
    "    if len(ls) >= 1:\n",
    "        if ls[0] == 'type':\n",
    "            comp = False\n",
    "    if len(ls) >= 1:\n",
    "        if ls[0] == 'location' or ls[0] == 'location':\n",
    "            # print(\"3\",ls)\n",
    "            if comp == True and det == True:\n",
    "                # print(\"2\",ls)\n",
    "                xyz = [float(ls[2]),float(ls[4]),float(ls[6])]\n",
    "                print(\"xyz\", xyz)\n",
    "                rot =float(ls[8]) \n",
    "                rot_xyz =[ float(ls[8]), float(ls[10]),float(ls[12]),float(ls[14])] \n",
    "                print(\"rotation of detector\",rot, rot_xyz) \n",
    "                d_list[len(d_list)-1].append(xyz)\n",
    "                rot_l1.append(rot_xyz)\n",
    "            elif comp == True and source == True: \n",
    "              source_pos = [float(ls[2]),float(ls[4]),float(ls[6])]\n",
    "            elif comp == True and sample == True: \n",
    "               sample_pos = np.array([float(ls[2]),float(ls[4]),float(ls[6])])\n",
    "# print(len(d_list))\n",
    "# print(\"sample and source position\",sample_pos,source_pos)\n",
    "# print(\"distance between sample and source\",CDist2(source_pos, sample_pos))\n",
    "# print(\"detector positions, relative to source at 0,0,0:\",d_list)\n",
    "# print(\"rotation list\",rot_l)\n",
    "#shift from rleative position to source to relative to sample\n",
    "ds_l = []\n",
    "sample_pos = sample_pos * [-1,-1,-1]\n",
    "print(\"sample_pos\",sample_pos)\n",
    "for i in range(len(d_list)):\n",
    "    det_pos = np.array([d_list[i][1][0],d_list[i][1][1],d_list[i][1][2]]) * [-1.0, -1.0,-1.0]\n",
    "    \n",
    "\n",
    "    rel_xyz= np.round(twoP_to_vec(sample_pos,det_pos),2)\n",
    "    print(\"rel_xyz\",rel_xyz)\n",
    "    rel_xyz = rel_xyz # * [-1,-1,-1]\n",
    "    print(\"sample to detector dist\",len3dvec(rel_xyz))\n",
    "    # rel_pos= twoP_to_vec(det_pos, sample_pos)\n",
    "    # print(\"detector position\",sample_pos,det_pos,rel_pos)\n",
    "    \n",
    "    # rel_xyz = [ d_list[i][1][0]-sample_pos[0], d_list[i][1][1]-sample_pos[1], (d_list[i][1][2]-sample_pos[2])]\n",
    "    \n",
    "    # print(\"relative position\",rel_xyz)\n",
    "    # rel_xyz = [ sample_pos[0]-d_list[i][1][0],sample_pos[1]-d_list[i][1][1], sample_pos[2]-d_list[i][1][2]]\n",
    "    # print(\"position\",rel_xyz,rel_pos)\n",
    "    # print(\"relative position\",rel_xyz)\n",
    "    ds_l.append(rel_xyz)\n",
    "vec_f = [-1,0,-0]\n",
    "vec_s = [0,-1,0]\n",
    "rot_l = [rot_l1[0],rot_l1[2],rot_l1[1]] #not corect but a temporary work around\n",
    "for i in range(len(rot_l)):\n",
    "    # if rot_l[i][2] < 0:\n",
    "        # theta = np.radians(rot_l[i][0]) \n",
    "    # else:\n",
    "    theta = np.radians(-rot_l[i][0])\n",
    "    v = [rot_l[i][1],rot_l[i][2],rot_l[i][3]]\n",
    "    v1 = [0,0,1]\n",
    "    print(\"v\",i,v,\"theta\", theta,rot_l[i][0])\n",
    "    fast_vec = np.round(np.dot(rotation_matrix(v,  theta), vec_f),2)\n",
    "    fast_v_round = np.array([np.round(fast_vec[0],1), np.round(fast_vec[1],1), np.round(fast_vec[2],1)])\n",
    "\n",
    "    fast_l.append(fast_vec)\n",
    "    slow_l.append(np.dot(rotation_matrix(v, theta), vec_s))\n",
    "    # slow_l.append([0,1,0])\n",
    "\n",
    "print(\"relative to sample position\",ds_l)\n",
    "print(\"fast axis\",fast_l)\n",
    "print(\"slow axis\", slow_l)\n",
    "print(rot_l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manual seeting of the detector gementrie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "# ds_l = [[ 0.25,  0.25, -0.29], [ 0.29,  0.25, -0.25], [-0.29,  0.25,  0.25]]\n",
    "# fast_l = [[-1.  ,  0.  , -0.01], [-0.01,  0.  ,  1.  ], [ 0.01,  0.  , -1.  ]]\n",
    "# slow_l = [[ 0., -1.,  0.], [ 0., -1.,  0.],[ 0., -1.,  0.]]\n",
    "\n",
    "\n",
    "ds_l = [[ -0.25,  -0.25, -0.29], [ -0.29,  -0.25, -0.25], [0.29,  -0.25,  0.25]]\n",
    "fast_l = [[1.  ,  0.  , -0.01], [0.01,  0.  ,  1.  ], [ -0.01,  0.  , -1.  ]]\n",
    "slow_l = [[ 0., 1.,  0.], [ 0., 1.,  0.],[ 0., 1.,  0.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gets crystal rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((f['entry1']['simulation']['Param'].keys()))\n",
    "print((f['entry1']['simulation']['Param']['XtalPhiX']))\n",
    "phix=float(list(str(f['entry1']['simulation']['Param']['XtalPhiX'][...][0]))[2])\n",
    "phiy=float(list(str(f['entry1']['simulation']['Param']['XtalPhiY'][...][0]))[2])\n",
    "phiz=float(list(str(f['entry1']['simulation']['Param']['XtalPhiZ'][...][0]))[2])\n",
    "#str(phix[0])\n",
    "#int(list(str(phix[0]))[2])\n",
    "print(phix,phiy,phiz)\n",
    "cor=[phix,phiy, phiz]\n",
    "cryst_or = np.array(cor)\n",
    "cryst_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generates output file name out of input filename and number of  time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = filename.split('/')\n",
    "print(no)\n",
    "name_out= no[-1].split('.')[0]\n",
    "print(name_out)\n",
    "file_out = '/Users/justinbergmann/work_flow/test_out/'+name_out+'_'+str(t_step)+'_out.h5'\n",
    "print(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_out, 'w') as fo:\n",
    "## create output nexus file\n",
    "   fo.attrs['default'] = 'NMX_data'\n",
    "   nxentry = fo.create_group('NMX_data')\n",
    "   nxentry.attrs[\"NX_class\"] = 'NXentry'\n",
    "   nxentry.attrs['default'] = 'data'\n",
    "   nxentry.attrs['name'] = 'NMX1'\n",
    "   #nxentry.__setitem__('beamline','NMX')\n",
    "   nxentry.__setitem__('name','NMX')\n",
    "   nxentry.__setitem__('definition','TOFRAW')\n",
    "   nxentry.attrs['name'] = \"NMX\"\n",
    "   #nxentry.attrs['name'].__setattr__('name','NMX') \n",
    "\n",
    "#SAMPLE\n",
    "   nx_sample = nxentry.create_group('NXsample')\n",
    "   nx_sample.__setitem__('name','Single_crystal')\n",
    "\n",
    "\n",
    "#Instrument\n",
    "   nx_instrument = nxentry.create_group('NXinstrument')\n",
    "\n",
    "   nx_detector = nxentry.create_group('NXdetector')\n",
    "   det_origen = nx_detector.create_dataset('origen', data=ds_l) \n",
    "   det_origen.attrs['units'] = 'm'\n",
    "\n",
    "   fast_axis = nx_detector.create_dataset('fast_axis', data=fast_l) \n",
    "   slow_axis = nx_detector.create_dataset('slow_axis', data=slow_l) \n",
    "\n",
    "   nx_beam = nxentry.create_group('NXbeam')\n",
    "\n",
    "\n",
    "   \n",
    "   proton = nxentry.create_dataset('proton_charge', data=weights.shape[0]/10000)    \n",
    "   \n",
    "   \n",
    "   nx_det1 = nxentry.create_group('detector_1')   \n",
    "   counts = nx_det1.create_dataset('counts', data=[group_t.values], compression=\"gzip\", compression_opts=4)\n",
    "\n",
    "   t_spectrum = nx_det1.create_dataset('t_bin', data=group_t.coords['t'].values, compression=\"gzip\", compression_opts=4)\n",
    "   t_spectrum.attrs['units'] = 'ms'\n",
    "   t_spectrum.attrs['long_name'] = 't_bin TOF (ms)'\n",
    "\n",
    "   pixel_id = nx_det1.create_dataset('pix_id', data=group_t.coords['id'].values, compression=\"gzip\", compression_opts=4)\n",
    "   pixel_id.attrs['units'] = ''\n",
    "   pixel_id.attrs['long_name'] = 'pixel ID'\n",
    "\n",
    "\n",
    "#SOURCE\n",
    "   nx_inst = nxentry.create_group('instrument')\n",
    "   nx_inst.attrs['nr_detector'] = len(fast_l)\n",
    "   nx_source = nxentry.create_group('NXsource')\n",
    "   nx_source.__setitem__('name','European Spallation Source')\n",
    "   nx_source.__setitem__('short_name','ESS')\n",
    "   nx_source.__setitem__('type','Spallation Neutron Source')\n",
    "   nx_source.__setitem__('distance',-(CDist2(source_pos, sample_pos)))\n",
    "   nx_source.__setitem__('probe','neutron')\n",
    "   nx_source.__setitem__('target_material','W')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    c_or = nxinst.create_dataset('crystal_orientation',data=cryst_or)\n",
    "#    c_or.attrs['unit'] = 'degrees'\n",
    "#    c_or.attrs['long_name'] = 'crystal orientation in Phi (degrees)'\n",
    "    \n",
    "\n",
    "   fo.close()\n",
    "   del fo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "delete some loaded lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "writes out some statistic of the reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print('2 current memory [MB]: {}, peak memory [MB]: {} '.format(round((current/(1024*1024)), 2), round((peak /(1024*1024) ), 2) ))\n",
    "# stopping the library\n",
    "# tracemalloc.stop()\n",
    "print(\"neded time (h:mm:ss): \", datetime.now() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f85ed83eabb820316696c474dce395f71e180a432bd3bf9cf71297a8fe85fff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
